{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd88ca5b-726a-492b-8bff-4e7c18dda389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using driver host: 172.22.0.3\n",
      "üöÄ Starting local to Docker Spark connection test...\n",
      "Waiting 10 seconds for cluster to be ready...\n",
      "Using driver host: 172.22.0.3\n",
      "‚úÖ Spark session created successfully!\n",
      "Testing basic RDD operations...\n",
      "‚úÖ Count test passed: 5\n",
      "‚úÖ Collect test passed: [1, 2, 3]\n",
      "‚úÖ Map test passed: [2, 4, 6]\n",
      "‚úÖ Sum test passed: 15\n",
      "\n",
      "üéâ All tests passed! Connection is working.\n",
      "\n",
      "üìä Cluster information:\n",
      "Master URL: spark://spark-master:7077\n",
      "Application ID: app-20251003053026-0000\n",
      "Spark UI: http://localhost:4040\n",
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import socket\n",
    "\n",
    "def get_host_ip():\n",
    "    \"\"\"Get host IP that containers can access\"\"\"\n",
    "    try:\n",
    "        # Connect to Spark master to determine reachable IP\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        s.connect((\"spark-master\", 7077))\n",
    "        ip = s.getsockname()[0]\n",
    "        s.close()\n",
    "        return ip\n",
    "    except:\n",
    "        # Fallback to host.docker.internal for Mac\n",
    "        return \"host.docker.internal\"\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"Create Spark session that works with Docker networking\"\"\"\n",
    "    \n",
    "    driver_host = get_host_ip()\n",
    "    print(f\"Using driver host: {driver_host}\")\n",
    "    \n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(\"SparkDockerFixed\")\n",
    "    conf.setMaster(\"spark://spark-master:7077\")\n",
    "    conf.set(\"spark.driver.host\", driver_host)\n",
    "    conf.set(\"spark.driver.port\", \"4050\")\n",
    "    conf.set(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    conf.set(\"spark.executor.memory\", \"1g\")\n",
    "    conf.set(\"spark.executor.cores\", \"1\")\n",
    "    conf.set(\"spark.network.timeout\", \"300s\")\n",
    "    conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    \n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    return spark\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "spark = create_spark_session()\n",
    "\n",
    "def simple_operation_test(spark):\n",
    "    \"\"\"Test simple operations without complex transformations\"\"\"\n",
    "    \n",
    "    print(\"Testing basic RDD operations...\")\n",
    "    \n",
    "    # Test 1: Simple count\n",
    "    try:\n",
    "        rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5], 2)  # 2 partitions\n",
    "        count = rdd.count()\n",
    "        print(f\"‚úÖ Count test passed: {count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Count test failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 2: Simple collect (avoid complex operations initially)\n",
    "    try:\n",
    "        rdd = spark.sparkContext.parallelize([1, 2, 3], 1)\n",
    "        data = rdd.collect()\n",
    "        print(f\"‚úÖ Collect test passed: {data}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Collect test failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 3: Simple map\n",
    "    try:\n",
    "        rdd = spark.sparkContext.parallelize([1, 2, 3], 1)\n",
    "        mapped = rdd.map(lambda x: x * 2)\n",
    "        result = mapped.collect()\n",
    "        print(f\"‚úÖ Map test passed: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Map test failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Only after basic tests work, try sum\n",
    "    try:\n",
    "        rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5], 2)\n",
    "        result = rdd.sum()\n",
    "        print(f\"‚úÖ Sum test passed: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Sum test failed: {e}\")\n",
    "        print(\"Trying alternative sum implementation...\")\n",
    "        \n",
    "        # Alternative approach for sum\n",
    "        try:\n",
    "            rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5], 2)\n",
    "            result = rdd.reduce(lambda a, b: a + b)\n",
    "            print(f\"‚úÖ Reduce sum passed: {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Reduce sum also failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ Starting local to Docker Spark connection test...\")\n",
    "    print(\"Waiting 10 seconds for cluster to be ready...\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    try:\n",
    "        spark = create_spark_session()\n",
    "        print(\"‚úÖ Spark session created successfully!\")\n",
    "        \n",
    "        # Test basic functionality\n",
    "        if simple_operation_test(spark):\n",
    "            print(\"\\nüéâ All tests passed! Connection is working.\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Some tests failed, but connection is established.\")\n",
    "        \n",
    "        # Show cluster info\n",
    "        print(f\"\\nüìä Cluster information:\")\n",
    "        print(f\"Master URL: {spark.sparkContext.master}\")\n",
    "        print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "        print(f\"Spark UI: http://localhost:4040\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create Spark session: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        try:\n",
    "            # spark.stop()\n",
    "            print(\"Spark session stopped.\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a25103-3c16-4d6a-8ea2-64b8d3a7440f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
