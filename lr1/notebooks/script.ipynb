{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd88ca5b-726a-492b-8bff-4e7c18dda389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using driver host: 172.22.0.3\n",
      "üöÄ Starting local to Docker Spark connection test...\n",
      "Waiting 10 seconds for cluster to be ready...\n",
      "Using driver host: 172.22.0.3\n",
      "‚ùå Failed to create Spark session: An error occurred while calling o64.applyModifiableSettings.\n",
      ": java.lang.IllegalStateException: LiveListenerBus is stopped.\n",
      "\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n",
      "\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n",
      "\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n",
      "\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n",
      "\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n",
      "\tat org.apache.spark.sql.SparkSession$.conf$lzycompute$1(SparkSession.scala:1213)\n",
      "\tat org.apache.spark.sql.SparkSession$.conf$1(SparkSession.scala:1213)\n",
      "\tat org.apache.spark.sql.SparkSession$.applyModifiableSettings(SparkSession.scala:1216)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Spark session stopped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_84/606157403.py\", line 101, in main\n",
      "    spark = create_spark_session()\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_84/606157403.py\", line 36, in create_spark_session\n",
      "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 504, in getOrCreate\n",
      "    ).applyModifiableSettings(session._jsparkSession, self._options)\n",
      "      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o64.applyModifiableSettings.\n",
      ": java.lang.IllegalStateException: LiveListenerBus is stopped.\n",
      "\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n",
      "\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n",
      "\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n",
      "\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n",
      "\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n",
      "\tat org.apache.spark.sql.SparkSession$.conf$lzycompute$1(SparkSession.scala:1213)\n",
      "\tat org.apache.spark.sql.SparkSession$.conf$1(SparkSession.scala:1213)\n",
      "\tat org.apache.spark.sql.SparkSession$.applyModifiableSettings(SparkSession.scala:1216)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import socket\n",
    "\n",
    "def get_host_ip():\n",
    "    \"\"\"Get host IP that containers can access\"\"\"\n",
    "    try:\n",
    "        # Connect to Spark master to determine reachable IP\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        s.connect((\"spark-master\", 7077))\n",
    "        ip = s.getsockname()[0]\n",
    "        s.close()\n",
    "        return ip\n",
    "    except:\n",
    "        # Fallback to host.docker.internal for Mac\n",
    "        return \"host.docker.internal\"\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"Create Spark session that works with Docker networking\"\"\"\n",
    "    \n",
    "    driver_host = get_host_ip()\n",
    "    print(f\"Using driver host: {driver_host}\")\n",
    "    \n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(\"SparkDockerFixed\")\n",
    "    conf.setMaster(\"spark://spark-master:7077\")\n",
    "    conf.set(\"spark.driver.host\", driver_host)\n",
    "    conf.set(\"spark.driver.port\", \"4050\")\n",
    "    conf.set(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    conf.set(\"spark.executor.memory\", \"1g\")\n",
    "    conf.set(\"spark.executor.cores\", \"1\")\n",
    "    conf.set(\"spark.network.timeout\", \"300s\")\n",
    "    conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    \n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    return spark\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "spark = create_spark_session()\n",
    "\n",
    "def simple_operation_test(spark):\n",
    "    \"\"\"Test simple operations without complex transformations\"\"\"\n",
    "    \n",
    "    print(\"Testing basic RDD operations...\")\n",
    "    \n",
    "    # Test 1: Simple count\n",
    "    try:\n",
    "        rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5], 2)  # 2 partitions\n",
    "        count = rdd.count()\n",
    "        print(f\"‚úÖ Count test passed: {count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Count test failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 2: Simple collect (avoid complex operations initially)\n",
    "    try:\n",
    "        rdd = spark.sparkContext.parallelize([1, 2, 3], 1)\n",
    "        data = rdd.collect()\n",
    "        print(f\"‚úÖ Collect test passed: {data}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Collect test failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 3: Simple map\n",
    "    try:\n",
    "        rdd = spark.sparkContext.parallelize([1, 2, 3], 1)\n",
    "        mapped = rdd.map(lambda x: x * 2)\n",
    "        result = mapped.collect()\n",
    "        print(f\"‚úÖ Map test passed: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Map test failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Only after basic tests work, try sum\n",
    "    try:\n",
    "        rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5], 2)\n",
    "        result = rdd.sum()\n",
    "        print(f\"‚úÖ Sum test passed: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Sum test failed: {e}\")\n",
    "        print(\"Trying alternative sum implementation...\")\n",
    "        \n",
    "        # Alternative approach for sum\n",
    "        try:\n",
    "            rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5], 2)\n",
    "            result = rdd.reduce(lambda a, b: a + b)\n",
    "            print(f\"‚úÖ Reduce sum passed: {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Reduce sum also failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ Starting local to Docker Spark connection test...\")\n",
    "    print(\"Waiting 10 seconds for cluster to be ready...\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    try:\n",
    "        spark = create_spark_session()\n",
    "        print(\"‚úÖ Spark session created successfully!\")\n",
    "        \n",
    "        # Test basic functionality\n",
    "        if simple_operation_test(spark):\n",
    "            print(\"\\nüéâ All tests passed! Connection is working.\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Some tests failed, but connection is established.\")\n",
    "        \n",
    "        # Show cluster info\n",
    "        print(f\"\\nüìä Cluster information:\")\n",
    "        print(f\"Master URL: {spark.sparkContext.master}\")\n",
    "        print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "        print(f\"Spark UI: http://localhost:4040\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create Spark session: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        try:\n",
    "            # spark.stop()\n",
    "            print(\"Spark session stopped.\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a25103-3c16-4d6a-8ea2-64b8d3a7440f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
